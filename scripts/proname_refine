#!/bin/bash

echo -ne "\n**************************************************************************************************************\n"
echo -ne "* You are executing the \033[1mproname_refine\033[0m script, which is the third part of the \033[1mPRONAME\033[0m pipeline.              *\n"
echo -ne "*                                                                                                            *\n"
echo -ne "* Please \033[31m\033[1mdo not\033[0m move, rename or remove files and folders generated by PRONAME until the end of the pipeline. *\n"
echo -ne "**************************************************************************************************************\n\n"

# Help menu

Help()
{

## Display Help
   echo -ne "\nThe proname_refine script is the third part of the PRONAME pipeline. This script carries out the clustering of high-quality sequence data coming from proname_filter, \nand the subsequent polishing with a tool dedicated to Nanopore data (i.e. medaka). Optionally, the script can import the generated files into QIIME2 for further processing."
   echo
   echo -ne "\nSyntax: proname_refine [--clusterid|--clusterthreads|--inputpath|--subsampledreads|--medakabatchsize|--medakathreads|--medakamodel|--chimeradb|--qiime2import|--version|--help]"
   echo
   echo "Options:"
   echo -ne "\t--clusterid\t\tThe percentage of identity at which clustering should be performed. [Option: decimal between 0 and 1]\n"
   echo
   echo -ne "\t--clusterthreads\tNumber of threads to use for the clustering step. You can know the number of available threads on your computer \n"
   echo -ne "\t\t\t\tby running the command 'nproc --all' [Default: 2]\n"
   echo
   echo -ne "\t--inputpath\t\tPath to the folder containing raw fastq files. This must be the same path than the one provided while \n"
   echo -ne "\t\t\t\trunning proname_import and proname_filter.\n"
   echo
   echo -ne "\t--subsampledreads\tNumber of subsampled reads that will be aligned against the centroid sequence during polishing. [Default: 300]\n"
   echo
   echo -ne "\t--medakabatchsize\tControls memory use. The default value has been set to 100. If Medaka shows out of memory errors, \n"
   echo -ne "\t\t\t\tthe batch size should be reduced. [Default: 100]\n"
   echo
   echo -ne "\t--medakathreads\t\tNumber of threads to use for the polishing step. This option determines the number of CPU threads \n"
   echo -ne "\t\t\t\tused for data loading and preprocessing, but not for consensus generation itself. Increasing this value beyond 2 \n"
   echo -ne "\t\t\t\tdoes not significantly improve performance because Medaka automatically uses additional threads for background \n"
   echo -ne "\t\t\t\tprocessing. If running Medaka on a CPU, setting --medakathreads to 2 is recommended. If running on a GPU, this \n"
   echo -ne "\t\t\t\tsetting has little impact, as the main computation is handled by PyTorch and CUDA. [Default: 2]\n"
   echo
   echo -ne "\t--medakamodel\t\tBasecalling model used to generate raw fastq files. This model will be used by medaka to polish data.\n"
   echo -ne "\t\t\t\tThe list of available models can be found by running 'medaka tools list\_models'\n"
   echo
   echo -ne "\t--chimeradb\t\tPath to the reference database to use for the chimera detection.\n"
   echo
   echo -ne "\t--qiime2import\t\tIndicate whether the generated representative sequences and table must be imported into QIIME2. [Option: \"yes\" or \"no\"]\n"
   echo
   echo -ne "\t--deletefiles\t\tDelete all non-essential files, i.e. files generated with proname_filter that are no more needed for the rest of \n"
   echo -ne "\t\t\t\tthe analysis through PRONAME. [Option: \"yes\" or \"no\", Default: no] \n"
   echo
   echo -ne "\t--version\t\tPrint the version of the pipeline.\n"
   echo
   echo -ne "\t--help\t\t\tPrint this help."
   echo


## Usage example
   echo -ne "\nUsage example:"
   echo -ne "\n-------------"
   echo
   echo "proname_refine \\"
   echo "  --clusterid 0.90 \\"
   echo "  --inputpath RawData \\"
   echo "  --medakamodel r1041_e82_400bps_sup_v5.0.0 \\"
   echo "  --chimeradb /opt/db/rEGEN-B/rEGEN-B_sequences.fasta \\"
   echo "  --qiime2import yes"
   echo
}

Version()
{
    echo -ne "proname_refine from the PRONAME pipeline, version 1.1.0\n\n"
}

clustering_threads="2"
n_subsampled_reads="300"
medaka_batchsize="100"
medaka_threads="2"
delete_non_essential="no"

# Transform long options to short ones

for arg in "$@"; do
  shift
  case "$arg" in
    '--help')               set -- "$@" '-h'   ;;
    '--version')            set -- "$@" '-v'   ;;
    '--clusterid')          set -- "$@" '-a'   ;;
    '--clusterthreads')     set -- "$@" '-b'   ;;
    '--inputpath')          set -- "$@" '-c'   ;;
    '--subsampledreads')    set -- "$@" '-d'   ;;
    '--medakabatchsize')    set -- "$@" '-e'   ;;
    '--medakathreads')      set -- "$@" '-f'   ;;
    '--medakamodel')        set -- "$@" '-g'   ;;
    '--chimeradb')          set -- "$@" '-i'   ;;
    '--qiime2import')       set -- "$@" '-j'   ;;
    '--deletefiles')        set -- "$@" '-r'   ;;
    *)                      set -- "$@" "$arg" ;;
  esac
done


# Parse short options

while getopts :hva:b:c:d:e:f:g:i:j:r: flag
do
    case "${flag}" in
        h) Help
        exit;;
        v) Version
        exit;;
        a) clustering_id=${OPTARG};;
        b) clustering_threads=${OPTARG};;
        c) fastq_folder=${OPTARG};;
        d) n_subsampled_reads=${OPTARG};;
        e) medaka_batchsize=${OPTARG};;
        f) medaka_threads=${OPTARG};;
        g) medaka_model=${OPTARG};;
        i) chimera_db=${OPTARG};;
        j) qiime2_import=${OPTARG};;
        r) delete_non_essential=${OPTARG};;
        \?) # Invalid option
                echo -ne "Error: Invalid option\n"
                echo -ne "Please consult the help menu with 'proname_refine --help'\n"
                exit;;
    esac
done


# Mandatory arguments

if [ ! "$clustering_id" ] || [ ! "$fastq_folder" ] || [ ! "$medaka_model" ] || [ ! "$chimera_db" ]  || [ ! "$qiime2_import" ]; then
  echo -ne "\n\033[31m\033[1mError: arguments --clusterid, --inputpath, --medakamodel, --chimeradb and --qiime2import must be provided\033[0m\n\n"
  echo -ne "Please consult the help menu with 'proname_refine --help'\n"
  exit;
fi

######################################################

# Listing the sequence identifiers present in each fastq file

if [ ! -d "Rawseqids" ]
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mRecording read distribution in each sample...\033[0m\n"

   mkdir Rawseqids

   for sample in $(ls ${fastq_folder} | awk -F '.fastq' '{print $1}')
   do
      awk 'NR == 1 || (NR-1) % 4 == 0' ${fastq_folder}/${sample}.fastq | cut -d "@" -f 2 > Rawseqids/rawseqids_${sample}
   done

   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mRead distribution recorded! \033[0m\n"
fi


# Storing in variables the path of files from proname_filter

## File from proname_filter with duplex data
if [ -e "HQ/HQ_duplex_seqs.fastq" ] && [ ! -e "HQ/HQ_simplex_duplex_seqs.fastq" ]
then
  HQ_fastq="HQ/HQ_duplex_seqs.fastq"
fi

## File from proname_filter with simplex data
if [ -e "HQ/HQ_simplex_seqs.fastq" ] && [ ! -e "HQ/HQ_simplex_duplex_seqs.fastq" ]
then
  HQ_fastq="HQ/HQ_simplex_seqs.fastq"
fi

## File from proname_filter with simplex and duplex data
if [ -f "HQ/HQ_simplex_duplex_seqs.fastq" ]
then
  HQ_fastq="HQ/HQ_simplex_duplex_seqs.fastq"
fi

# Removing items from a potential previous failed run

rm -rf vsearch_clusters
rm -f cluster*_seq_count

# Clustering reads into OTUs

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mClustering reads...\033[0m\n"

mkdir vsearch_clusters

vsearch \
  --cluster_fast ${HQ_fastq} \
  --id ${clustering_id} \
  --strand both \
  --threads ${clustering_threads} \
  --clusters vsearch_clusters/cluster

remove_singletons.py vsearch_clusters


# Recording clustered reads according to their sample provenance

for cluster in $(ls vsearch_clusters)
do

   cluster_seqs=$(grep ">" vsearch_clusters/${cluster} | cut -d ">" -f 2)

   for sample in $(ls ${fastq_folder} | awk -F '.fastq' '{print $1}')
   do
      paste <(printf %s ${sample}) <(grep -c -f <(echo $cluster_seqs | sed 's/ /\n/g') Rawseqids/rawseqids_${sample})
   done > ${cluster}_seq_count
done

if find vsearch_clusters -name 'cluster*' | grep -q . 1> /dev/null 2>&1
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mRead clustering completed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: read clustering failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Splitting clusters into 2 files

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mSplitting clusters into two files...\033[0m\n"

for i in vsearch_clusters/*;do seqkit split -p 2 $i;done

if find vsearch_clusters -name '*split*' | grep -q . 1> /dev/null 2>&1
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mSplitting completed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: cluster splitting failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Extracting centroid sequences

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mExtracting centroid sequences...\033[0m\n"

for i in $(ls vsearch_clusters | grep "split" | cut -d "." -f 1)
do
   seqkit range -r -1:-1 vsearch_clusters/${i}.split/${i}.part_001 -w 0 -o vsearch_clusters/centroid_${i}.fasta
done

if find vsearch_clusters -name 'centroid*' | grep -q . 1> /dev/null 2>&1
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mExtraction completed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: centroid extraction failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Randomly subsampling N reads

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mSubsampling reads in each cluster...\033[0m\n"

for i in $(ls vsearch_clusters | grep "split" | cut -d "." -f 1)
do
   seqkit shuffle -s 23 vsearch_clusters/$i 2>/dev/null | seqkit head -n ${n_subsampled_reads} -w 0 -o vsearch_clusters/subreads_${i}.fasta
done

if find vsearch_clusters -name 'subreads*' | grep -q . 1> /dev/null 2>&1
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mSubsampling completed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: read subsampling failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Generating consensus sequences

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mCarrying out medaka polishing to generate consensus sequences...\033[0m\n"

for i in $(ls vsearch_clusters | grep "split" | cut -d "." -f 1)
do
   medaka_consensus \
      -d vsearch_clusters/centroid_${i}.fasta \
      -i vsearch_clusters/subreads_${i}.fasta \
      -b ${medaka_batchsize} \
      -t ${medaka_threads} \
      -m ${medaka_model} \
      -o vsearch_clusters/medaka_${i}
done

if find vsearch_clusters -name 'medaka*' | grep -q . 1> /dev/null 2>&1
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mPolishing completed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: polishing failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Creating a linking table between clusters and their corresponding representative seqid

for i in $(ls vsearch_clusters | grep "medaka" | cut -d "_" -f 2)
do
   paste \
      <(printf %s $i) \
      <(grep ">" vsearch_clusters/medaka_${i}/consensus.fasta | cut -d ">" -f 2)
done > clusters_seqids_linking_table


# Creating the feature table
## Adding a header line in each seq_count file

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mCreating a feature table...\033[0m\n"

for i in $(ls | grep "seq_count" | cut -d "_" -f 1)
do
   echo -ne "Feature_ID\t$(grep "$i" clusters_seqids_linking_table | cut -d $'\t' -f 2)\n$(cat ${i}_seq_count)\n" > ${i}_seq_count_tmp
   mv ${i}_seq_count_tmp ${i}_seq_count
done


## Merging all seq_count files

for i in $(ls | grep "seq_count" | head -n 1)
do
   cut -d $'\t' -f 1 $i
done > Global_table

for i in *_seq_count
do 
   awk 'BEGIN {FS=OFS="\t"} NR==FNR{a[$1]=$2;next}{ for (i in a) if ($1 == i) {$(NF+1)=a[i]}}1' $i Global_table > Global_table_tmp && mv Global_table_tmp Global_table
done


## Inverting lines and columns

awk -F '\t' '{for (i=1; i<=NF; i++) {if (NR == 1) {col[i] = $i} else {col[i] = col[i] "\t" $i}}} END {for (i=1; col[i] != ""; i++) {print col[i]}}' Global_table > Global_table_tmp && mv Global_table_tmp Global_table

if [ -e "Global_table" ]
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mFeature table created! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: the creation of the feature table failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Removing chimera sequences

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mRemoving chimera sequences...\033[0m\n"

cat vsearch_clusters/medaka*/consensus.fasta > medaka_consensus_seqs.fasta

vsearch \
  --uchime_ref medaka_consensus_seqs.fasta \
  --db ${chimera_db} \
  --nonchimeras rep_seqs.fasta

if [ -e "rep_seqs.fasta" ]
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mChimera sequences removed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: the creation of the feature table failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Checking that every nucleotide is uppercase

awk '{if ($0 ~ /^>/) {print $0} else {print toupper($0)}}' rep_seqs.fasta > rep_seqs.fasta_tmp
mv rep_seqs.fasta_tmp rep_seqs.fasta

echo -ne "$(head -n 1 Global_table)\n$(grep -f <(grep ">" rep_seqs.fasta | cut -d ">" -f 2) Global_table)\n" > rep_table.tsv


# Clean-up

rm *_seq_count
rm clusters_seqids_linking_table
rm Global_table
rm medaka_consensus_seqs.fasta
rm -r vsearch_clusters

if [ ${qiime2_import} = "yes" ]
then
   # Data import into QIIME2
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mImporting representative sequences and table into QIIME2...\033[0m\n"

   biom convert -i rep_table.tsv -o rep_table.biom --table-type "OTU table" --to-hdf5

   qiime tools import \
     --input-path rep_table.biom \
     --type 'FeatureTable[Frequency]' \
     --input-format BIOMV210Format \
     --output-path rep_table.qza > /dev/null 2>/dev/null

   rm rep_table.biom

   qiime tools import \
     --type FeatureData[Sequence] \
     --input-path rep_seqs.fasta \
     --output-path rep_seqs.qza > /dev/null 2>/dev/null

   echo -ne "sample-id\tsample-name\n#q2:types\tcategorical\n$(paste <(ls ${fastq_folder} | awk -F '.fastq' '{print $1}') <(ls ${fastq_folder} | awk -F '.fastq' '{print $1}'))\n" > sample_metadata.tsv

   if [ -e "rep_table.qza" ] && [ -e "rep_seqs.qza" ]
   then
      echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mImport completed! \033[0m\n"
   else
      echo -ne "\n\033[31m\033[1mError: data import into QIIME2 failed. \033[0m\n\n"
      echo -ne "Please consult the help menu.\n"
      exit;
   fi

   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : The next and last script of the pipeline to use is \033[1mproname_taxonomy\033[0m.\n\n"
   echo -ne "\nUsage example:"
   echo -ne "\n-------------"
   echo
   echo "proname_taxonomy \\"
   echo "  --qseqs rep_seqs.qza \\"
   echo "  --qtable rep_table.qza \\"
   echo "  --db /opt/db/rEGEN-B/rEGEN-B_sequences.fasta \\"
   echo "  --reftax /opt/db/rEGEN-B/rEGEN-B_taxonomy.tsv \\"
   echo "  --metadata sample_metadata.tsv \\"
   echo "  --assay assay1 \\"
   echo "  --phyloseq yes"
   echo

else
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : The next and last script of the pipeline to use is \033[1mproname_taxonomy\033[0m.\n\n"
   echo -ne "\nUsage example:"
   echo -ne "\n-------------"
   echo
   echo "proname_taxonomy \\"
   echo "  --qseqs rep_seqs.fasta \\"
   echo "  --qtable rep_table.tsv \\"
   echo "  --db /opt/db/rEGEN-B/rEGEN-B_sequences.fasta \\"
   echo "  --reftax /opt/db/rEGEN-B/rEGEN-B_taxonomy.tsv \\"
   echo "  --assay assay1 \\"
   echo "  --phyloseq yes"
   echo
fi

if [ ${delete_non_essential} = "yes" ]
then
    rm *read_distribution.tsv
    rm LengthvsQualityScatterPlot*
    rm -r HQ
fi


unset HQ_fastq
unset clustering_id
unset clustering_threads
unset fastq_folder
unset n_subsampled_reads
unset medaka_batchsize
unset medaka_threads
unset medaka_model
unset chimera_db
unset qiime2_import
unset delete_non_essential

